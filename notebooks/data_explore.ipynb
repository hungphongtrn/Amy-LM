{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40178f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hungphongtrn/Workspace/Amy-LM/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"/home/hungphongtrn/Workspace/Amy-LM/data/Amy-LM-Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d9f4bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 197/197 [00:01<00:00, 111.78 examples/s]? shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.41s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  354MB /  354MB, 1.49MB/s  \n",
      "New Data Upload: 100%|██████████|  294MB /  294MB, 1.49MB/s  \n",
      "Map: 100%|██████████| 197/197 [00:01<00:00, 116.86 examples/s]02, 107.22s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.35s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  353MB /  353MB, 1.44MB/s  \n",
      "New Data Upload: 100%|██████████|  353MB /  353MB, 1.44MB/s  \n",
      "Map: 100%|██████████| 197/197 [00:01<00:00, 126.69 examples/s]33, 76.80s/ shards] \n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:09<00:00,  1.50s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  365MB /  365MB, 5.40MB/s  \n",
      "New Data Upload: 100%|██████████|  365MB /  365MB, 5.40MB/s  \n",
      "Map: 100%|██████████| 197/197 [00:01<00:00, 120.20 examples/s]51, 66.61s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.46s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  362MB /  362MB, 5.20MB/s  \n",
      "New Data Upload: 100%|██████████|  362MB /  362MB, 5.20MB/s  \n",
      "Map: 100%|██████████| 197/197 [00:01<00:00, 110.30 examples/s]17, 60.71s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.42s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  348MB /  348MB, 4.76MB/s  \n",
      "New Data Upload: 100%|██████████|  348MB /  348MB, 4.76MB/s  \n",
      "Map: 100%|██████████| 197/197 [00:01<00:00, 113.70 examples/s]15, 58.14s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.43s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  352MB /  352MB,  965kB/s  \n",
      "New Data Upload: 100%|██████████|  352MB /  352MB,  965kB/s  \n",
      "Map: 100%|██████████| 197/197 [00:01<00:00, 118.89 examples/s]40, 59.14s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.45s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  350MB /  350MB, 4.41MB/s  \n",
      "New Data Upload: 100%|██████████|  350MB /  350MB, 4.41MB/s  \n",
      "Map: 100%|██████████| 197/197 [00:01<00:00, 109.62 examples/s]10, 57.76s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.44s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  364MB /  364MB, 2.17MB/s  \n",
      "New Data Upload: 100%|██████████|  364MB /  364MB, 2.17MB/s  \n",
      "Map: 100%|██████████| 197/197 [00:01<00:00, 100.00 examples/s]24, 58.30s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.35s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  345MB /  345MB, 2.79MB/s  \n",
      "New Data Upload: 100%|██████████|  345MB /  345MB, 2.79MB/s  \n",
      "Map: 100%|██████████| 197/197 [00:01<00:00, 110.17 examples/s]12, 57.62s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.39s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  348MB /  348MB, 4.41MB/s  \n",
      "New Data Upload: 100%|██████████|  295MB /  295MB, 4.41MB/s  \n",
      "Map: 100%|██████████| 197/197 [00:01<00:00, 104.51 examples/s]:40, 55.84s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.44s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  360MB /  360MB, 4.17MB/s  \n",
      "New Data Upload: 100%|██████████|  360MB /  360MB, 4.17MB/s  \n",
      "Map: 100%|██████████| 197/197 [00:01<00:00, 125.01 examples/s]:49, 56.08s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.41s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  354MB /  354MB, 3.34MB/s  \n",
      "New Data Upload: 100%|██████████|  354MB /  354MB, 3.34MB/s  \n",
      "Map: 100%|██████████| 197/197 [00:01<00:00, 114.97 examples/s]:30, 54.73s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.36s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  344MB /  344MB, 2.87MB/s  \n",
      "New Data Upload: 100%|██████████|  344MB /  344MB, 2.87MB/s  \n",
      "Map: 100%|██████████| 197/197 [00:01<00:00, 104.76 examples/s]:22, 53.89s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.39s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  343MB /  343MB, 1.08MB/s  \n",
      "New Data Upload: 100%|██████████|  343MB /  343MB, 1.08MB/s  \n",
      "Map: 100%|██████████| 197/197 [00:02<00:00, 96.78 examples/s]3:36, 54.43s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.37s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  339MB /  339MB, 2.46MB/s  \n",
      "New Data Upload: 100%|██████████|  339MB /  339MB, 2.46MB/s  \n",
      "Map: 100%|██████████| 197/197 [00:02<00:00, 98.10 examples/s] :53, 55.24s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.48s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  355MB /  355MB, 5.07MB/s  \n",
      "New Data Upload: 100%|██████████|  355MB /  355MB, 5.07MB/s  \n",
      "Map: 100%|██████████| 196/196 [00:01<00:00, 103.36 examples/s]:58, 55.25s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.41s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  350MB /  350MB, 4.52MB/s  \n",
      "New Data Upload: 100%|██████████|  303MB /  303MB, 4.52MB/s  \n",
      "Map: 100%|██████████| 196/196 [00:01<00:00, 108.18 examples/s]:02, 55.20s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.41s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  349MB /  349MB, 7.93MB/s  \n",
      "New Data Upload: 100%|██████████|  349MB /  349MB, 7.93MB/s  \n",
      "Map: 100%|██████████| 196/196 [00:02<00:00, 83.97 examples/s]9:36, 52.43s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.39s/ba]\n",
      "Processing Files (0 / 1): 100%|█████████▉|  348MB /  348MB, 12.5MB/s  \n",
      "New Data Upload: 100%|██████████|  348MB /  348MB, 12.5MB/s  \n",
      "Map: 100%|██████████| 196/196 [00:01<00:00, 103.78 examples/s]:52, 47.30s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:09<00:00,  1.51s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  369MB /  369MB, 4.38MB/s  \n",
      "New Data Upload: 100%|██████████|  369MB /  369MB, 4.38MB/s  \n",
      "Map: 100%|██████████| 196/196 [00:01<00:00, 119.41 examples/s]:46, 45.12s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.41s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  341MB /  341MB, 4.14MB/s  \n",
      "New Data Upload: 100%|██████████|  341MB /  341MB, 4.14MB/s  \n",
      "Map: 100%|██████████| 196/196 [00:01<00:00, 101.50 examples/s]:51, 43.88s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.42s/ba]\n",
      "Processing Files (0 / 1): 100%|█████████▉|  345MB /  345MB, 7.65MB/s  \n",
      "New Data Upload: 100%|██████████|  345MB /  345MB, 7.65MB/s  \n",
      "Map: 100%|██████████| 196/196 [00:01<00:00, 99.54 examples/s]5:01, 43.04s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.47s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  358MB /  358MB, 4.66MB/s  \n",
      "New Data Upload: 100%|██████████|  358MB /  358MB, 4.66MB/s  \n",
      "Map: 100%|██████████| 196/196 [00:01<00:00, 100.29 examples/s]:20, 43.41s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.43s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  343MB /  343MB, 11.3MB/s  \n",
      "New Data Upload: 100%|██████████|  343MB /  343MB, 11.3MB/s  \n",
      "Map: 100%|██████████| 196/196 [00:01<00:00, 117.51 examples/s]:24, 41.00s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:07<00:00,  1.33s/ba]\n",
      "Processing Files (0 / 1): 100%|█████████▉|  335MB /  335MB, 5.13MB/s  \n",
      "New Data Upload: 100%|██████████|  335MB /  335MB, 5.13MB/s  \n",
      "Map: 100%|██████████| 196/196 [00:01<00:00, 99.50 examples/s] :41, 40.48s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.47s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  369MB /  369MB, 15.9MB/s  \n",
      "New Data Upload: 100%|██████████|  369MB /  369MB, 15.9MB/s  \n",
      "Map: 100%|██████████| 196/196 [00:02<00:00, 82.31 examples/s]1:57, 39.10s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:09<00:00,  1.55s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  366MB /  366MB, 8.76MB/s  \n",
      "New Data Upload: 100%|██████████|  366MB /  366MB, 8.76MB/s  \n",
      "Map: 100%|██████████| 196/196 [00:01<00:00, 113.84 examples/s]:17, 38.69s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.48s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  365MB /  365MB, 3.36MB/s  \n",
      "New Data Upload: 100%|██████████|  306MB /  306MB, 3.36MB/s  \n",
      "Map: 100%|██████████| 196/196 [00:01<00:00, 109.41 examples/s]:40, 40.81s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:08<00:00,  1.45s/ba]\n",
      "Processing Files (1 / 1): 100%|██████████|  351MB /  351MB, 6.22MB/s  \n",
      "New Data Upload: 100%|██████████|  300MB /  300MB, 6.22MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 29/29 [24:29<00:00, 50.66s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/hungphongtrn/Amy-LM-Dataset/commit/e0c2398845b76524e0d7b4a950267d4b30a53865', commit_message='Upload dataset', commit_description='', oid='e0c2398845b76524e0d7b4a950267d4b30a53865', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/hungphongtrn/Amy-LM-Dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='hungphongtrn/Amy-LM-Dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.push_to_hub(\"hungphongtrn/Amy-LM-Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d914b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "segment_id = ds[\"segment_id\"][0]\n",
    "llm_features = np.array(ds[\"llm_feat\"][0])\n",
    "llm_times = np.array(ds[\"llm_times\"][0])\n",
    "wavlm_features = np.array(ds[\"wavlm_feat\"][0])\n",
    "audio = ds[\"audio\"][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "991664d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_id YOU1000000044_S0000363\n",
      "llm_features (32, 2048)\n",
      "llm_times [[ 0.    0.24]\n",
      " [ 0.    0.24]\n",
      " [ 0.    0.24]\n",
      " [ 0.    0.24]\n",
      " [ 0.24  0.4 ]\n",
      " [ 0.4   0.56]\n",
      " [ 0.4   0.56]\n",
      " [ 0.4   0.56]\n",
      " [ 0.4   0.56]\n",
      " [ 0.56  0.72]\n",
      " [ 0.72  0.96]\n",
      " [ 0.72  0.96]\n",
      " [ 0.96  1.12]\n",
      " [ 1.12  1.28]\n",
      " [ 1.28  1.52]\n",
      " [ 1.28  1.52]\n",
      " [ 1.52  1.84]\n",
      " [ 1.84  2.  ]\n",
      " [ 2.    2.16]\n",
      " [ 2.16  2.32]\n",
      " [ 2.32  2.4 ]\n",
      " [ 2.4   2.56]\n",
      " [ 2.4   2.56]\n",
      " [ 2.56  2.64]\n",
      " [ 2.56  2.64]\n",
      " [ 2.56  2.64]\n",
      " [ 2.56  2.64]\n",
      " [ 2.64  2.88]\n",
      " [ 2.88 -1.  ]\n",
      " [ 2.88 -1.  ]\n",
      " [ 2.88 -1.  ]\n",
      " [ 2.88 -1.  ]]\n",
      "wavlm_features (713, 1024)\n",
      "audio AudioSamples:\n",
      "  data (shape): torch.Size([1, 48800])\n",
      "  pts_seconds: 0.0\n",
      "  duration_seconds: 3.05\n",
      "  sample_rate: 16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"segment_id\", segment_id)\n",
    "print(\"llm_features\", llm_features.shape)\n",
    "print(\"llm_times\", llm_times)\n",
    "print(\"wavlm_features\", wavlm_features.shape)\n",
    "print(\"audio\", audio.get_all_samples())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a234e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def process_batch(batch, target_fps=25):\n",
    "    \"\"\"\n",
    "    Processing function compatible with ds.map(..., batched=True).\n",
    "    \"\"\"\n",
    "    batch_size = len(batch['segment_id'])\n",
    "    \n",
    "    # Initialize output lists\n",
    "    out_llm_feats = []\n",
    "    out_wavlm_feats = []\n",
    "    out_lengths = []\n",
    "    \n",
    "    # Iterate through each sample in the batch\n",
    "    for i in range(batch_size):\n",
    "        # -------------------------------------------------------------\n",
    "        # 1. Setup Individual Item Data\n",
    "        # -------------------------------------------------------------\n",
    "        # Audio info\n",
    "        audio_array = batch['audio'][i]['array']\n",
    "        sr = batch['audio'][i]['sampling_rate']\n",
    "        duration = len(audio_array) / sr\n",
    "        \n",
    "        # Calculate target frames for this specific item\n",
    "        num_frames = int(np.ceil(duration * target_fps))\n",
    "        out_lengths.append(num_frames)\n",
    "\n",
    "        # Create Time Grid (A and B)\n",
    "        frame_indices = np.arange(num_frames)\n",
    "        grid_starts_A = frame_indices / target_fps\n",
    "        grid_ends_B = (frame_indices + 1) / target_fps\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # 2. Process LLM Features (Integral Image Alignment)\n",
    "        # -------------------------------------------------------------\n",
    "        llm_feat = np.array(batch['llm_feat'][i])\n",
    "        llm_times = np.array(batch['llm_times'][i])\n",
    "\n",
    "        # Fix -1 in end times (replace with actual duration)\n",
    "        # Note: We must copy to avoid mutating the cached dataset in place unexpectedly\n",
    "        t_ends = llm_times[:, 1].copy()\n",
    "        t_ends[t_ends == -1] = duration\n",
    "        t_starts = llm_times[:, 0]\n",
    "\n",
    "        # --- Vectorized Binary Search ---\n",
    "        \n",
    "        # A: Closest smaller start time -> First token with that start\n",
    "        idx_closest_start = np.searchsorted(t_starts, grid_starts_A, side='right') - 1\n",
    "        idx_closest_start = np.clip(idx_closest_start, 0, len(t_starts) - 1)\n",
    "        val_closest_start = t_starts[idx_closest_start]\n",
    "        final_idx_starts = np.searchsorted(t_starts, val_closest_start, side='left')\n",
    "\n",
    "        # B: Closest larger end time -> Last token with that end\n",
    "        idx_closest_end = np.searchsorted(t_ends, grid_ends_B, side='left')\n",
    "        idx_closest_end = np.clip(idx_closest_end, 0, len(t_ends) - 1)\n",
    "        val_closest_end = t_ends[idx_closest_end]\n",
    "        final_idx_ends = np.searchsorted(t_ends, val_closest_end, side='right') - 1\n",
    "\n",
    "        # --- Integral Image Pooling ---\n",
    "        # Prefix sum (pad with 0 at start)\n",
    "        feat_cumsum = np.vstack([np.zeros((1, llm_feat.shape[1])), np.cumsum(llm_feat, axis=0)])\n",
    "        \n",
    "        # Sum = Cumulative[End+1] - Cumulative[Start]\n",
    "        # We ensure indices are valid for the cumsum array\n",
    "        sums = feat_cumsum[final_idx_ends + 1] - feat_cumsum[final_idx_starts]\n",
    "        \n",
    "        # Mean = Sum / Count\n",
    "        counts = (final_idx_ends - final_idx_starts + 1).reshape(-1, 1)\n",
    "        counts = np.maximum(counts, 1) # Prevent division by zero\n",
    "        aligned_llm = sums / counts\n",
    "        \n",
    "        out_llm_feats.append(aligned_llm.astype(np.float32))\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # 3. Process WavLM Features (Adaptive Pooling)\n",
    "        # -------------------------------------------------------------\n",
    "        wavlm_tensor = torch.tensor(batch['wavlm_feat'][i]).float() # (Src_Len, 1024)\n",
    "        \n",
    "        # Transpose to (1, Channel, Time) for pooling\n",
    "        wavlm_tensor = wavlm_tensor.transpose(0, 1).unsqueeze(0)\n",
    "        \n",
    "        # Pool to exact number of frames\n",
    "        wavlm_aligned = torch.nn.functional.adaptive_avg_pool1d(wavlm_tensor, num_frames)\n",
    "        \n",
    "        # Transpose back: (Time, Channel)\n",
    "        wavlm_aligned = wavlm_aligned.squeeze(0).transpose(0, 1)\n",
    "        \n",
    "        out_wavlm_feats.append(wavlm_aligned.numpy())\n",
    "\n",
    "    # Return dictionary of lists (updates the dataset columns)\n",
    "    return {\n",
    "        \"llm_feat\": out_llm_feats,      # List of (T, 2048)\n",
    "        \"wavlm_feat\": out_wavlm_feats,  # List of (T, 1024)\n",
    "        \"num_frames\": out_lengths       # List of ints\n",
    "    }\n",
    "\n",
    "# --- How to apply ---\n",
    "# updated_ds = ds.map(\n",
    "#     lambda x: process_batch(x, target_fps=25), \n",
    "#     batched=True, \n",
    "#     batch_size=32, \n",
    "#     num_proc=4,   # Safe to use multiprocessing with this logic\n",
    "#     remove_columns=[\"audio\", \"llm_times\"] # Optional: clean up columns you don't need\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3431188",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_output = process_batch(ds[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8c8bd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_feat': [array([[ 14.856689 ,  -9.355469 ,  14.222656 , ..., -13.0217285,\n",
       "          -12.165039 ,   7.4833984],\n",
       "         [ 14.856689 ,  -9.355469 ,  14.222656 , ..., -13.0217285,\n",
       "          -12.165039 ,   7.4833984],\n",
       "         [ 14.856689 ,  -9.355469 ,  14.222656 , ..., -13.0217285,\n",
       "          -12.165039 ,   7.4833984],\n",
       "         ...,\n",
       "         [ 27.441406 ,  11.6953125,  14.611328 , ..., -43.984375 ,\n",
       "          -23.296875 ,  -1.7578125],\n",
       "         [ 27.441406 ,  11.6953125,  14.611328 , ..., -43.984375 ,\n",
       "          -23.296875 ,  -1.7578125],\n",
       "         [ 27.441406 ,  11.6953125,  14.611328 , ..., -43.984375 ,\n",
       "          -23.296875 ,  -1.7578125]], shape=(77, 2048), dtype=float32)],\n",
       " 'wavlm_feat': [array([[ 0.04122524,  0.19869384, -0.13108978, ..., -0.10124207,\n",
       "           0.06408997, -0.1493988 ],\n",
       "         [ 0.09528179, -0.07081299, -0.18926391, ..., -0.07943535,\n",
       "          -0.1532257 ,  0.14864807],\n",
       "         [ 0.01480007, -0.16293183,  0.06273498, ..., -0.02133637,\n",
       "          -0.08496094,  0.31502074],\n",
       "         ...,\n",
       "         [-0.06931152, -0.26457518,  0.10352783, ..., -0.36972657,\n",
       "           0.22382812, -0.01263008],\n",
       "         [-0.08054809, -0.25786132,  0.11507569, ..., -0.3864258 ,\n",
       "           0.21523437, -0.00474091],\n",
       "         [-0.06851806, -0.22189942,  0.09713745, ..., -0.3326416 ,\n",
       "           0.23706055,  0.01084633]], shape=(77, 1024), dtype=float32)],\n",
       " 'num_frames': [77]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33586ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
