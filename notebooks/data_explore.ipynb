{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47afea40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hungphongtrn/Workspace/Amy-LM/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wavlm_feature = load_dataset(\"hungphongtrn/wavlm-features\", split=\"dev\")\n",
    "lllm_hidden_states = load_dataset(\"hungphongtrn/llm-features\", split=\"dev\")\n",
    "gigaspeech = load_dataset(\"fixie-ai/gigaspeech\", \"dev\", split=\"dev\")\n",
    "speech_time_alignment = load_dataset(\"hungphongtrn/speech-time-alignment\", split=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0b75b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioSamples:\n",
       "  data (shape): torch.Size([1, 48800])\n",
       "  pts_seconds: 0.0\n",
       "  duration_seconds: 3.05\n",
       "  sample_rate: 16000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gigaspeech['audio'][0].get_all_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b204e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([713, 1024])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load WavLM features\n",
    "import torch\n",
    "\n",
    "wavlm_feat_sample = torch.tensor(wavlm_feature['wavlm_feat'][0], dtype=torch.float16)\n",
    "wavlm_feat_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03a02736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio\n",
    "import torchaudio\n",
    "\n",
    "audio_data = gigaspeech['audio'][0].get_all_samples().data\n",
    "resampled_audio = torchaudio.functional.resample(audio_data, gigaspeech['audio'][0].get_all_samples().sample_rate, 24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1956ac02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 73200])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1799cf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2048])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load LLM features\n",
    "llm_feat_sample = torch.tensor(lllm_hidden_states['llm_feat'][0], dtype=torch.float16)\n",
    "llm_feat_sample.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d0dd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'char': ['So'],\n",
       "  'start_offset': 3,\n",
       "  'end_offset': 5,\n",
       "  'start': 0.24,\n",
       "  'end': 0.4},\n",
       " {'char': ['I'],\n",
       "  'start_offset': 7,\n",
       "  'end_offset': 9,\n",
       "  'start': 0.56,\n",
       "  'end': 0.72},\n",
       " {'char': ['don'],\n",
       "  'start_offset': 9,\n",
       "  'end_offset': 10,\n",
       "  'start': 0.72,\n",
       "  'end': 0.8},\n",
       " {'char': [\"'\"],\n",
       "  'start_offset': 10,\n",
       "  'end_offset': 10,\n",
       "  'start': 0.8,\n",
       "  'end': 0.8},\n",
       " {'char': ['t'],\n",
       "  'start_offset': 11,\n",
       "  'end_offset': 12,\n",
       "  'start': 0.88,\n",
       "  'end': 0.96},\n",
       " {'char': ['know'],\n",
       "  'start_offset': 12,\n",
       "  'end_offset': 14,\n",
       "  'start': 0.96,\n",
       "  'end': 1.12},\n",
       " {'char': ['if'],\n",
       "  'start_offset': 14,\n",
       "  'end_offset': 16,\n",
       "  'start': 1.12,\n",
       "  'end': 1.28},\n",
       " {'char': ['there'],\n",
       "  'start_offset': 16,\n",
       "  'end_offset': 17,\n",
       "  'start': 1.28,\n",
       "  'end': 1.36},\n",
       " {'char': [\"'\"],\n",
       "  'start_offset': 17,\n",
       "  'end_offset': 17,\n",
       "  'start': 1.36,\n",
       "  'end': 1.36},\n",
       " {'char': ['s'],\n",
       "  'start_offset': 18,\n",
       "  'end_offset': 19,\n",
       "  'start': 1.44,\n",
       "  'end': 1.52},\n",
       " {'char': ['somet'],\n",
       "  'start_offset': 19,\n",
       "  'end_offset': 21,\n",
       "  'start': 1.52,\n",
       "  'end': 1.68},\n",
       " {'char': ['hing'],\n",
       "  'start_offset': 21,\n",
       "  'end_offset': 23,\n",
       "  'start': 1.68,\n",
       "  'end': 1.84},\n",
       " {'char': ['going'],\n",
       "  'start_offset': 23,\n",
       "  'end_offset': 25,\n",
       "  'start': 1.84,\n",
       "  'end': 2.0},\n",
       " {'char': ['on'],\n",
       "  'start_offset': 25,\n",
       "  'end_offset': 27,\n",
       "  'start': 2.0,\n",
       "  'end': 2.16},\n",
       " {'char': ['at'],\n",
       "  'start_offset': 27,\n",
       "  'end_offset': 29,\n",
       "  'start': 2.16,\n",
       "  'end': 2.32},\n",
       " {'char': ['the'],\n",
       "  'start_offset': 29,\n",
       "  'end_offset': 30,\n",
       "  'start': 2.32,\n",
       "  'end': 2.4},\n",
       " {'char': ['moment'],\n",
       "  'start_offset': 30,\n",
       "  'end_offset': 32,\n",
       "  'start': 2.4,\n",
       "  'end': 2.56},\n",
       " {'char': [','],\n",
       "  'start_offset': 32,\n",
       "  'end_offset': 32,\n",
       "  'start': 2.56,\n",
       "  'end': 2.56},\n",
       " {'char': ['but'],\n",
       "  'start_offset': 33,\n",
       "  'end_offset': 36,\n",
       "  'start': 2.64,\n",
       "  'end': 2.88}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "sample_alignment = json.loads(speech_time_alignment[0]['alignment_json'])\n",
    "timestamp_aligned = sample_alignment['char']\n",
    "timestamp_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f3e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([713, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def align_wavlm_features(features):\n",
    "    \"\"\"\n",
    "    Downsamples WavLM features from 50Hz to 12.5Hz (4x reduction)\n",
    "    using Average Pooling.\n",
    "    \n",
    "    Args:\n",
    "        features (torch.Tensor): Shape (Time, Dim) or (Batch, Time, Dim).\n",
    "                                 Example: [713, 1024]\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Downsampled features.\n",
    "                      Example: [178, 1024]\n",
    "    \"\"\"\n",
    "    # 1. Handle unbatched input (Time, Dim) -> (1, Time, Dim)\n",
    "    is_unbatched = features.dim() == 2\n",
    "    if is_unbatched:\n",
    "        features = features.unsqueeze(0)\n",
    "    \n",
    "    # 2. Permute to (Batch, Dim, Time) for PyTorch Pooling\n",
    "    # Input is currently (Batch, Time, Dim)\n",
    "    features_transposed = features.permute(0, 2, 1)\n",
    "    \n",
    "    # 3. Apply Average Pooling\n",
    "    # kernel_size=4, stride=4 performs the 4x downsampling\n",
    "    pooled = F.avg_pool1d(features_transposed, kernel_size=4, stride=4)\n",
    "    \n",
    "    # 4. Permute back to (Batch, Time, Dim)\n",
    "    output = pooled.permute(0, 2, 1)\n",
    "    \n",
    "    # 5. Remove batch dim if input was unbatched\n",
    "    if is_unbatched:\n",
    "        output = output.squeeze(0)\n",
    "        \n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7516f00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
