{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40178f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"/home/hungphongtrn/Workspace/Amy-LM/data/Amy-LM-Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d9f4bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['segment_id', 'llm_feat', 'llm_times', 'wavlm_feat', 'audio'],\n",
       "    num_rows: 5700\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d914b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "segment_id = ds[\"segment_id\"][0]\n",
    "llm_features = np.array(ds[\"llm_feat\"][0])\n",
    "llm_times = np.array(ds[\"llm_times\"][0])\n",
    "wavlm_features = np.array(ds[\"wavlm_feat\"][0])\n",
    "audio = ds[\"audio\"][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "991664d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_id YOU1000000044_S0000363\n",
      "llm_features (32, 2048)\n",
      "llm_times [[ 0.    0.24]\n",
      " [ 0.    0.24]\n",
      " [ 0.    0.24]\n",
      " [ 0.    0.24]\n",
      " [ 0.24  0.4 ]\n",
      " [ 0.4   0.56]\n",
      " [ 0.4   0.56]\n",
      " [ 0.4   0.56]\n",
      " [ 0.4   0.56]\n",
      " [ 0.56  0.72]\n",
      " [ 0.72  0.96]\n",
      " [ 0.72  0.96]\n",
      " [ 0.96  1.12]\n",
      " [ 1.12  1.28]\n",
      " [ 1.28  1.52]\n",
      " [ 1.28  1.52]\n",
      " [ 1.52  1.84]\n",
      " [ 1.84  2.  ]\n",
      " [ 2.    2.16]\n",
      " [ 2.16  2.32]\n",
      " [ 2.32  2.4 ]\n",
      " [ 2.4   2.56]\n",
      " [ 2.4   2.56]\n",
      " [ 2.56  2.64]\n",
      " [ 2.56  2.64]\n",
      " [ 2.56  2.64]\n",
      " [ 2.56  2.64]\n",
      " [ 2.64  2.88]\n",
      " [ 2.88 -1.  ]\n",
      " [ 2.88 -1.  ]\n",
      " [ 2.88 -1.  ]\n",
      " [ 2.88 -1.  ]]\n",
      "wavlm_features (713, 1024)\n",
      "audio AudioSamples:\n",
      "  data (shape): torch.Size([1, 48800])\n",
      "  pts_seconds: 0.0\n",
      "  duration_seconds: 3.05\n",
      "  sample_rate: 16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"segment_id\", segment_id)\n",
    "print(\"llm_features\", llm_features.shape)\n",
    "print(\"llm_times\", llm_times)\n",
    "print(\"wavlm_features\", wavlm_features.shape)\n",
    "print(\"audio\", audio.get_all_samples())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a234e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def process_batch(batch, target_fps=25):\n",
    "    \"\"\"\n",
    "    Processing function compatible with ds.map(..., batched=True).\n",
    "    \"\"\"\n",
    "    batch_size = len(batch['segment_id'])\n",
    "    \n",
    "    # Initialize output lists\n",
    "    out_llm_feats = []\n",
    "    out_wavlm_feats = []\n",
    "    out_lengths = []\n",
    "    \n",
    "    # Iterate through each sample in the batch\n",
    "    for i in range(batch_size):\n",
    "        # -------------------------------------------------------------\n",
    "        # 1. Setup Individual Item Data\n",
    "        # -------------------------------------------------------------\n",
    "        # Audio info\n",
    "        audio_array = batch['audio'][i]['array']\n",
    "        sr = batch['audio'][i]['sampling_rate']\n",
    "        duration = len(audio_array) / sr\n",
    "        \n",
    "        # Calculate target frames for this specific item\n",
    "        num_frames = int(np.ceil(duration * target_fps))\n",
    "        out_lengths.append(num_frames)\n",
    "\n",
    "        # Create Time Grid (A and B)\n",
    "        frame_indices = np.arange(num_frames)\n",
    "        grid_starts_A = frame_indices / target_fps\n",
    "        grid_ends_B = (frame_indices + 1) / target_fps\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # 2. Process LLM Features (Integral Image Alignment)\n",
    "        # -------------------------------------------------------------\n",
    "        llm_feat = np.array(batch['llm_feat'][i])\n",
    "        llm_times = np.array(batch['llm_times'][i])\n",
    "\n",
    "        # Fix -1 in end times (replace with actual duration)\n",
    "        # Note: We must copy to avoid mutating the cached dataset in place unexpectedly\n",
    "        t_ends = llm_times[:, 1].copy()\n",
    "        t_ends[t_ends == -1] = duration\n",
    "        t_starts = llm_times[:, 0]\n",
    "\n",
    "        # --- Vectorized Binary Search ---\n",
    "        \n",
    "        # A: Closest smaller start time -> First token with that start\n",
    "        idx_closest_start = np.searchsorted(t_starts, grid_starts_A, side='right') - 1\n",
    "        idx_closest_start = np.clip(idx_closest_start, 0, len(t_starts) - 1)\n",
    "        val_closest_start = t_starts[idx_closest_start]\n",
    "        final_idx_starts = np.searchsorted(t_starts, val_closest_start, side='left')\n",
    "\n",
    "        # B: Closest larger end time -> Last token with that end\n",
    "        idx_closest_end = np.searchsorted(t_ends, grid_ends_B, side='left')\n",
    "        idx_closest_end = np.clip(idx_closest_end, 0, len(t_ends) - 1)\n",
    "        val_closest_end = t_ends[idx_closest_end]\n",
    "        final_idx_ends = np.searchsorted(t_ends, val_closest_end, side='right') - 1\n",
    "\n",
    "        # --- Integral Image Pooling ---\n",
    "        # Prefix sum (pad with 0 at start)\n",
    "        feat_cumsum = np.vstack([np.zeros((1, llm_feat.shape[1])), np.cumsum(llm_feat, axis=0)])\n",
    "        \n",
    "        # Sum = Cumulative[End+1] - Cumulative[Start]\n",
    "        # We ensure indices are valid for the cumsum array\n",
    "        sums = feat_cumsum[final_idx_ends + 1] - feat_cumsum[final_idx_starts]\n",
    "        \n",
    "        # Mean = Sum / Count\n",
    "        counts = (final_idx_ends - final_idx_starts + 1).reshape(-1, 1)\n",
    "        counts = np.maximum(counts, 1) # Prevent division by zero\n",
    "        aligned_llm = sums / counts\n",
    "        \n",
    "        out_llm_feats.append(aligned_llm.astype(np.float32))\n",
    "\n",
    "        # -------------------------------------------------------------\n",
    "        # 3. Process WavLM Features (Adaptive Pooling)\n",
    "        # -------------------------------------------------------------\n",
    "        wavlm_tensor = torch.tensor(batch['wavlm_feat'][i]).float() # (Src_Len, 1024)\n",
    "        \n",
    "        # Transpose to (1, Channel, Time) for pooling\n",
    "        wavlm_tensor = wavlm_tensor.transpose(0, 1).unsqueeze(0)\n",
    "        \n",
    "        # Pool to exact number of frames\n",
    "        wavlm_aligned = torch.nn.functional.adaptive_avg_pool1d(wavlm_tensor, num_frames)\n",
    "        \n",
    "        # Transpose back: (Time, Channel)\n",
    "        wavlm_aligned = wavlm_aligned.squeeze(0).transpose(0, 1)\n",
    "        \n",
    "        out_wavlm_feats.append(wavlm_aligned.numpy())\n",
    "\n",
    "    # Return dictionary of lists (updates the dataset columns)\n",
    "    return {\n",
    "        \"llm_feat\": out_llm_feats,      # List of (T, 2048)\n",
    "        \"wavlm_feat\": out_wavlm_feats,  # List of (T, 1024)\n",
    "        \"num_frames\": out_lengths       # List of ints\n",
    "    }\n",
    "\n",
    "# --- How to apply ---\n",
    "# updated_ds = ds.map(\n",
    "#     lambda x: process_batch(x, target_fps=25), \n",
    "#     batched=True, \n",
    "#     batch_size=32, \n",
    "#     num_proc=4,   # Safe to use multiprocessing with this logic\n",
    "#     remove_columns=[\"audio\", \"llm_times\"] # Optional: clean up columns you don't need\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3431188",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_output = process_batch(ds[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8c8bd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_feat': [array([[ 14.856689 ,  -9.355469 ,  14.222656 , ..., -13.0217285,\n",
       "          -12.165039 ,   7.4833984],\n",
       "         [ 14.856689 ,  -9.355469 ,  14.222656 , ..., -13.0217285,\n",
       "          -12.165039 ,   7.4833984],\n",
       "         [ 14.856689 ,  -9.355469 ,  14.222656 , ..., -13.0217285,\n",
       "          -12.165039 ,   7.4833984],\n",
       "         ...,\n",
       "         [ 27.441406 ,  11.6953125,  14.611328 , ..., -43.984375 ,\n",
       "          -23.296875 ,  -1.7578125],\n",
       "         [ 27.441406 ,  11.6953125,  14.611328 , ..., -43.984375 ,\n",
       "          -23.296875 ,  -1.7578125],\n",
       "         [ 27.441406 ,  11.6953125,  14.611328 , ..., -43.984375 ,\n",
       "          -23.296875 ,  -1.7578125]], shape=(77, 2048), dtype=float32)],\n",
       " 'wavlm_feat': [array([[ 0.04122524,  0.19869384, -0.13108978, ..., -0.10124207,\n",
       "           0.06408997, -0.1493988 ],\n",
       "         [ 0.09528179, -0.07081299, -0.18926391, ..., -0.07943535,\n",
       "          -0.1532257 ,  0.14864807],\n",
       "         [ 0.01480007, -0.16293183,  0.06273498, ..., -0.02133637,\n",
       "          -0.08496094,  0.31502074],\n",
       "         ...,\n",
       "         [-0.06931152, -0.26457518,  0.10352783, ..., -0.36972657,\n",
       "           0.22382812, -0.01263008],\n",
       "         [-0.08054809, -0.25786132,  0.11507569, ..., -0.3864258 ,\n",
       "           0.21523437, -0.00474091],\n",
       "         [-0.06851806, -0.22189942,  0.09713745, ..., -0.3326416 ,\n",
       "           0.23706055,  0.01084633]], shape=(77, 1024), dtype=float32)],\n",
       " 'num_frames': [77]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33586ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
