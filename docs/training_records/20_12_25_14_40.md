# Training Records

## **1. Current Status Analysis (Epoch 10)**

The model is currently in an **adversarial fine-tuning phase** with reconstruction loss (`msspec`) disabled.

* **Generator Loss Breakdown ():**
* **Adversarial Component ():** This accounts for approximately **65%** of the total generator loss. The generator is successfully "fighting" the discriminator.
* **Feature Matching ():** This is very low, suggesting the generator's internal representations are already well-aligned with the discriminator's view of real audio.
* **LLM Distillation ():** With a cosine distance of ~0.086, the semantic codebook has achieved a high cosine similarity (~0.914) with the LLM features. This suggests the semantic distillation is working effectively.
* **WavLM Distillation ():** This is the **most significant non-adversarial loss**. A cosine distance of 0.693 implies a similarity of only ~0.307. The model is struggling to capture prosodic/acoustic information as defined by WavLM.
* **Reconstruction Loss (`msspec`):** Currently  as `adversarial_only` is set to `True`.


* **Discriminator Status ():**
In a "hinge" GAN setup (used here), a discriminator loss around 1.7-1.8 is relatively high. This indicates the discriminator is not yet "overpowering" the generator, which is generally good for stability, but it may also mean the discriminator needs more capacity or training to provide a sharper gradient for audio quality.

---

## **2. Suggested Improvements**

## **A. Enable Multi-Scale Mel Spectrogram Loss (`msspec`)**

Even in a "fine-tuning" phase, keeping a small weight on the `msspec` loss is critical.

* **Reason:** Without it, the generator may focus entirely on "tricking" the discriminator and matching distillation targets, leading to artifacts or a loss of phase/pitch accuracy that the discriminator might not catch immediately.
* **Action:** Change `adversarial_only=False` in `train.py` and potentially lower `alpha_msspec` to  or  if you want to prioritize distillation.

## **B. Address the WavLM Distillation Gap**

The high WavLM loss (0.693) suggests the model cannot effectively map its latent space to WavLM's 1024-dimensional space using the current projection.

* **Reason:** The code uses a simple `nn.Linear` for `wavlm_proj`. WavLM features are complex and contain both phonetic and speaker-level information.
* **Action:** 1.  **Upgrade Projections:** Replace the linear layers in `CompressorTrainer` with a small MLP (e.g., Linear -> ReLU -> Linear) to provide more capacity for the latent mapping.
2.  **Increase Weight:** If WavLM capture is a priority for prosody, increase `alpha_wavlm` to .

## **C. Balance the GAN Loss Weights**

The adversarial loss () is currently dominating the distillation losses.

* **Action:** If the audio sounds "robotic" or lacks the intended "prosody" from WavLM, consider reducing `alpha_adv` to  or  while increasing distillation weights. This forces the model to prioritize the information content of the codebooks over pure "realism" as judged by the discriminator.

## **D. Gradient Clipping and Optimization**

* **Observation:** You are manually calling `clip_gradients` for the generator but the discriminator's gradient clipping is not explicitly shown in the `training_step` for `train_adv`.
* **Action:** Ensure the discriminator also benefits from gradient clipping to prevent spikes during the adversarial phase, especially since the hinge loss can sometimes produce large gradients if the discriminator falls behind.

## **E. Qualitative Monitoring**

* **Action:** Since `msspec` is , the standard loss metrics won't tell you if the audio is intelligible. Implement a callback to save audio samples (input vs. reconstructed) to Weights & Biases (Wandb) every epoch. This is the only way to verify if the low `llm_loss` actually translates to intelligible semantic reconstruction.